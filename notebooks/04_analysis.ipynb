{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Model Analysis and Optimization\n",
    "SHAP analysis for interpretability and hyperparameter tuning for improved performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_header",
   "metadata": {},
   "source": [
    "## Load Data and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "X_train_scaled = np.load('../data/processed/X_train_scaled.npy')\n",
    "X_test_scaled = np.load('../data/processed/X_test_scaled.npy')\n",
    "y_train = np.load('../data/processed/y_train.npy')\n",
    "y_test = np.load('../data/processed/y_test.npy')\n",
    "\n",
    "# Load feature names\n",
    "with open('../data/processed/feature_names.pkl', 'rb') as f:\n",
    "    feature_names = pickle.load(f)\n",
    "\n",
    "# Load best model\n",
    "with open('../data/processed/best_model.pkl', 'rb') as f:\n",
    "    best_model = pickle.load(f)\n",
    "\n",
    "# Load best model name\n",
    "with open('../data/processed/best_model_name.pkl', 'rb') as f:\n",
    "    best_model_name = pickle.load(f)\n",
    "\n",
    "# Load results\n",
    "results_df = pd.read_csv('../data/processed/model_results.csv')\n",
    "\n",
    "# Create DataFrame for feature names\n",
    "X = pd.DataFrame(X_test_scaled, columns=feature_names)\n",
    "\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(f\"Test set shape: {X_test_scaled.shape}\")\n",
    "print(f\"Features: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shap_header",
   "metadata": {},
   "source": [
    "## SHAP Analysis\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) provides interpretability by showing how each feature contributes to predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shap_compute",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing SHAP explainer...\")\n",
    "\n",
    "sample_size = min(1000, len(X_test_scaled))\n",
    "sample_indices = np.random.choice(len(X_test_scaled), sample_size, replace=False)\n",
    "X_test_sample = X_test_scaled[sample_indices]\n",
    "\n",
    "try:\n",
    "    if best_model_name in ['Random Forest', 'Gradient Boosting', 'Decision Tree']:\n",
    "        explainer = shap.TreeExplainer(best_model)\n",
    "    else:\n",
    "        train_sample_indices = np.random.choice(len(X_train_scaled), 100, replace=False)\n",
    "        explainer = shap.KernelExplainer(best_model.predict_proba, X_train_scaled[train_sample_indices])\n",
    "\n",
    "    shap_values = explainer.shap_values(X_test_sample)\n",
    "\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[1]\n",
    "\n",
    "    print(\"SHAP values computed successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"SHAP error: {e}\")\n",
    "    shap_values = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shap_viz_header",
   "metadata": {},
   "source": [
    "### SHAP Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shap_bar",
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_values is not None:\n",
    "    X_test_sample_df = pd.DataFrame(X_test_sample, columns=feature_names)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values, X_test_sample_df, plot_type=\"bar\", show=False)\n",
    "    plt.title('SHAP Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/figures/shap_importance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shap_summary_header",
   "metadata": {},
   "source": [
    "### SHAP Summary Plot\n",
    "\n",
    "Shows the distribution of SHAP values for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shap_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_values is not None:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values, X_test_sample_df, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/figures/shap_summary.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shap_analysis_header",
   "metadata": {},
   "source": [
    "### SHAP Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shap_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "if shap_values is not None:\n",
    "    # Calculate mean absolute SHAP values\n",
    "    shap_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Mean_SHAP': np.abs(shap_values).mean(axis=0)\n",
    "    }).sort_values('Mean_SHAP', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 20 Features by SHAP Importance:\")\n",
    "    print(shap_importance.head(20).to_string(index=False))\n",
    "    \n",
    "    # Save SHAP importance\n",
    "    shap_importance.to_csv('../results/figures/shap_importance.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tune_header",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Optimizing the best model's hyperparameters using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tune_params",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid based on best model\n",
    "if best_model_name == 'Gradient Boosting':\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.05, 0.1, 0.15],\n",
    "        'max_depth': [3, 5, 7]\n",
    "    }\n",
    "    model_class = GradientBoostingClassifier\n",
    "elif best_model_name == 'Random Forest':\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 15, 20],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "    model_class = RandomForestClassifier\n",
    "else:\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    param_grid = {\n",
    "        'C': [0.01, 0.1, 1.0, 10.0],\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['lbfgs']\n",
    "    }\n",
    "    model_class = LogisticRegression\n",
    "\n",
    "print(f\"Tuning {best_model_name}...\")\n",
    "print(f\"Parameter grid: {param_grid}\")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    model_class(random_state=RANDOM_STATE),\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV F1 score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tuned_eval_header",
   "metadata": {},
   "source": [
    "### Evaluate Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tuned_eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model = grid_search.best_estimator_\n",
    "y_pred_tuned = tuned_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "tuned_accuracy = accuracy_score(y_test, y_pred_tuned)\n",
    "tuned_precision = precision_score(y_test, y_pred_tuned)\n",
    "tuned_recall = recall_score(y_test, y_pred_tuned)\n",
    "tuned_f1 = f1_score(y_test, y_pred_tuned)\n",
    "\n",
    "if hasattr(tuned_model, 'predict_proba'):\n",
    "    tuned_proba = tuned_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    tuned_roc_auc = roc_auc_score(y_test, tuned_proba)\n",
    "else:\n",
    "    tuned_roc_auc = None\n",
    "\n",
    "# Compare with original\n",
    "original_f1 = results_df.iloc[0]['F1']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON: Original vs Tuned\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n{'Metric':<15} {'Original':<12} {'Tuned':<12} {'Improvement'}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Accuracy':<15} {results_df.iloc[0]['Accuracy']:<12.4f} {tuned_accuracy:<12.4f} {'+' if tuned_accuracy > results_df.iloc[0]['Accuracy'] else ''}{(tuned_accuracy - results_df.iloc[0]['Accuracy']):.4f}\")\n",
    "print(f\"{'Precision':<15} {results_df.iloc[0]['Precision']:<12.4f} {tuned_precision:<12.4f} {'+' if tuned_precision > results_df.iloc[0]['Precision'] else ''}{(tuned_precision - results_df.iloc[0]['Precision']):.4f}\")\n",
    "print(f\"{'Recall':<15} {results_df.iloc[0]['Recall']:<12.4f} {tuned_recall:<12.4f} {'+' if tuned_recall > results_df.iloc[0]['Recall'] else ''}{(tuned_recall - results_df.iloc[0]['Recall']):.4f}\")\n",
    "print(f\"{'F1-Score':<15} {original_f1:<12.4f} {tuned_f1:<12.4f} {'+' if tuned_f1 > original_f1 else ''}{(tuned_f1 - original_f1):.4f}\")\n",
    "if tuned_roc_auc is not None:\n",
    "    print(f\"{'ROC-AUC':<15} {results_df.iloc[0]['ROC-AUC']:<12.4f} {tuned_roc_auc:<12.4f} {'+' if tuned_roc_auc > results_df.iloc[0]['ROC-AUC'] else ''}{(tuned_roc_auc - results_df.iloc[0]['ROC-AUC']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final_header",
   "metadata": {},
   "source": [
    "## Final Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the better model (tuned vs original)\n",
    "final_model = tuned_model if tuned_f1 > original_f1 else best_model\n",
    "final_predictions = tuned_model.predict(X_test_scaled)\n",
    "\n",
    "print(f\"\\nFinal Model: {best_model_name} {'(Tuned)' if tuned_f1 > original_f1 else '(Original)'}\")\n",
    "print(f\"Training Samples: {len(y_train):,}\")\n",
    "print(f\"Test Samples: {len(y_test):,}\")\n",
    "\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "print(f\"  Accuracy:  {accuracy_score(y_test, final_predictions):.4f}\")\n",
    "print(f\"  Precision: {precision_score(y_test, final_predictions):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_test, final_predictions):.4f}\")\n",
    "print(f\"  F1-Score:  {f1_score(y_test, final_predictions):.4f}\")\n",
    "\n",
    "if hasattr(final_model, 'predict_proba'):\n",
    "    final_proba = final_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    print(f\"  ROC-AUC:   {roc_auc_score(y_test, final_proba):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_final_header",
   "metadata": {},
   "source": [
    "## Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_final",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "with open('../data/processed/final_model.pkl', 'wb') as f:\n",
    "    pickle.dump(final_model, f)\n",
    "\n",
    "print(\"Final model saved to ../data/processed/final_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insights_header",
   "metadata": {},
   "source": [
    "## Key Insights and Findings\n",
    "\n",
    "Based on machine learning analysis of the SHED 2024 respondents, the following factors\n",
    "most strongly predict inability to cover a $400 emergency expense:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insights_content",
   "metadata": {},
   "source": [
    "### 1. HOUSEHOLD COMPOSITION\n",
    "Larger household size (pphhsize: -0.15 correlation) and having children under 18 (ppkid017: -0.13 correlation) are major barriers to emergency fund access. Family size amplifies financial fragility.\n",
    "\n",
    "### 2. AGE & STABILITY\n",
    "Older age (ppage: +0.25) and years in US (pphi0018: +0.17) strongly predict ability to cover emergencies. Housing stability through rent (R3: +0.25) or mortgage payments (M4: +0.12) indicates financial capacity.\n",
    "\n",
    "### 3. EMPLOYMENT & CHILDCARE\n",
    "Childcare costs (CG2: +0.17) paradoxically correlate with ability to cover emergencies, suggesting employment enables both childcare payment and emergency savings.\n",
    "\n",
    "### INTERSECTIONAL BARRIERS: The \"Glass Wallet\" Effect\n",
    "- Young families with multiple children face triple burden: more expenses, less time to build savings, higher cost of living\n",
    "- Geographic inequality: 41.8% nationally cannot cover $400, but state variation shows systemic disparities (lowest vs highest state differs by 20+ percentage points)\n",
    "- Data gaps in minimum wage (only 11 states with specific data) hide how many Americans rely on federal $7.25 minimum, masking true extent of wage inadequacy\n",
    "\n",
    "### GEOGRAPHIC DISPARITIES\n",
    "- States with lowest emergency fund access show intersection of low minimum wages, high cost of living, and limited banking access\n",
    "- Real income growth (income_pct_change: +0.036) shows modest positive effect, but state-level aggregates don't capture individual variation\n",
    "- Cost of living adjustments (RPP data) reveal that nominal income doesn't equal purchasing power across states\n",
    "\n",
    "### MODEL PERFORMANCE\n",
    "- Gradient Boosting achieved 83.6% accuracy, 0.859 F1, 0.910 ROC-AUC\n",
    "- Successfully identifies 86% of those who can cover emergencies\n",
    "- Balanced precision/recall shows model doesn't systematically favor one class\n",
    "\n",
    "### POLICY IMPLICATIONS\n",
    "- Emergency savings programs should target young families with children\n",
    "- Geographic wage disparities require localized minimum wage policies\n",
    "- Banking access and financial literacy programs needed in low-coverage states\n",
    "- Data collection must improve to reveal hidden barriers (complete minimum wage data, individual-level economic indicators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion_header",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This analysis successfully identified the key predictors of financial fragility and demonstrated that machine learning models can accurately predict emergency fund access with over 83% accuracy. The SHAP analysis revealed that demographic factors—particularly household composition and age—compound to create the \"Glass Wallet\" effect where certain populations face significantly greater barriers to financial security.\n",
    "\n",
    "The model's strong performance and interpretability make it a valuable tool for:\n",
    "1. Targeting financial assistance programs\n",
    "2. Understanding systemic barriers to financial security\n",
    "3. Informing policy decisions around minimum wage and cost of living adjustments\n",
    "4. Identifying at-risk populations for early intervention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
