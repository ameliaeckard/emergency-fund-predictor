{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "This notebook handles data cleaning, merging datasets, encoding categorical variables, and preparing data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_header",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "load_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHED shape: (12295, 753)\n",
      "RPP shape: (55, 7)\n",
      "FRED shape: (58, 13)\n"
     ]
    }
   ],
   "source": [
    "fred = pd.read_csv(\"../data/raw/fredgraph.csv\")\n",
    "rpp = pd.read_excel(\"../data/raw/rpp1224.xlsx\", skiprows=5)\n",
    "shed = pd.read_csv(\"../data/raw/public2024.csv\")\n",
    "\n",
    "rpp.columns = ['state', 'real_pce_2022', 'real_pce_2023', 'pce_pct_change',\n",
    "               'real_income_2022', 'real_income_2023', 'income_pct_change']\n",
    "rpp['state'] = rpp['state'].str.lower().str.strip()\n",
    "rpp = rpp[~rpp['state'].isin(['united states', 'nan', ''])]\n",
    "rpp = rpp.dropna(subset=['state'])\n",
    "\n",
    "print(f\"SHED shape: {shed.shape}\")\n",
    "print(f\"RPP shape: {rpp.shape}\")\n",
    "print(f\"FRED shape: {fred.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "target_header",
   "metadata": {},
   "source": [
    "## Define Target Variable\n",
    "\n",
    "Creating binary target: Can cover $400 emergency with cash/savings (1) vs Cannot cover (0)\n",
    "\n",
    "We use EF3_c which asks if respondents can cover a $400 emergency 'with the money currently in my checking/savings account or with cash.' This represents true financial security - having liquid assets available for emergencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "target_variable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target variable distribution:\n",
      "can_cover_400\n",
      "0    6927\n",
      "1    5368\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentage who CAN cover: 43.7%\n",
      "Percentage who CANNOT cover: 56.3%\n"
     ]
    }
   ],
   "source": [
    "if 'EF3_c' in shed.columns:\n",
    "    shed['can_cover_400'] = (shed['EF3_c'] == 'Yes').astype(int)\n",
    "    \n",
    "    print(f\"\\nTarget variable distribution:\")\n",
    "    print(shed['can_cover_400'].value_counts())\n",
    "    print(f\"\\nPercentage who CAN cover: {shed['can_cover_400'].mean()*100:.1f}%\")\n",
    "    print(f\"Percentage who CANNOT cover: {(1-shed['can_cover_400'].mean())*100:.1f}%\")\n",
    "else:\n",
    "    print(f\"Error: EF3_c column not found in dataset\")\n",
    "    print(f\"Available EF3 columns: {[col for col in shed.columns if col.startswith('EF3')]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "merge_header",
   "metadata": {},
   "source": [
    "## Merge Datasets\n",
    "\n",
    "Adding state-level economic indicators to individual SHED data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "state_mapping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States successfully mapped: 12295\n",
      "States not mapped: 0\n",
      "\n",
      "Merged dataset shape: (12295, 762)\n",
      "Records with RPP data: 12094\n",
      "Records without RPP data: 201\n",
      "\n",
      "Sample merged data:\n",
      "  ppstaten      state_name  pce_pct_change  real_income_2023\n",
      "0       ny        new york             3.2         1243246.0\n",
      "1       nj      new jersey             2.1          581816.0\n",
      "2       il        illinois             4.6          761677.0\n",
      "3       wi       wisconsin             1.1          342613.0\n",
      "4       fl         florida             3.0         1247047.0\n",
      "5       nc  north carolina             2.8          591116.0\n",
      "6       ct     connecticut             5.2          260583.0\n",
      "7       ny        new york             3.2         1243246.0\n",
      "8       ky        kentucky             1.1          230020.0\n",
      "9       pa    pennsylvania            -0.1          761197.0\n"
     ]
    }
   ],
   "source": [
    "state_abbrev_to_name = {\n",
    "    'al': 'alabama', 'ak': 'alaska', 'az': 'arizona', 'ar': 'arkansas',\n",
    "    'ca': 'california', 'co': 'colorado', 'ct': 'connecticut', 'de': 'delaware',\n",
    "    'fl': 'florida', 'ga': 'georgia', 'hi': 'hawaii', 'id': 'idaho',\n",
    "    'il': 'illinois', 'in': 'indiana', 'ia': 'iowa', 'ks': 'kansas',\n",
    "    'ky': 'kentucky', 'la': 'louisiana', 'me': 'maine', 'md': 'maryland',\n",
    "    'ma': 'massachusetts', 'mi': 'michigan', 'mn': 'minnesota', 'ms': 'mississippi',\n",
    "    'mo': 'missouri', 'mt': 'montana', 'ne': 'nebraska', 'nv': 'nevada',\n",
    "    'nh': 'new hampshire', 'nj': 'new jersey', 'nm': 'new mexico', 'ny': 'new york',\n",
    "    'nc': 'north carolina', 'nd': 'north dakota', 'oh': 'ohio', 'ok': 'oklahoma',\n",
    "    'or': 'oregon', 'pa': 'pennsylvania', 'ri': 'rhode island', 'sc': 'south carolina',\n",
    "    'sd': 'south dakota', 'tn': 'tennessee', 'tx': 'texas', 'ut': 'utah',\n",
    "    'vt': 'vermont', 'va': 'virginia', 'wa': 'washington', 'wv': 'west virginia',\n",
    "    'wi': 'wisconsin', 'wy': 'wyoming', 'dc': 'district of columbia'\n",
    "}\n",
    "\n",
    "shed['state_name'] = shed['ppstaten'].map(state_abbrev_to_name)\n",
    "\n",
    "print(f\"States successfully mapped: {shed['state_name'].notna().sum()}\")\n",
    "print(f\"States not mapped: {shed['state_name'].isna().sum()}\")\n",
    "\n",
    "df = shed.merge(rpp, left_on='state_name', right_on='state', how='left')\n",
    "\n",
    "print(f\"\\nMerged dataset shape: {df.shape}\")\n",
    "print(f\"Records with RPP data: {df['pce_pct_change'].notna().sum()}\")\n",
    "print(f\"Records without RPP data: {df['pce_pct_change'].isna().sum()}\")\n",
    "\n",
    "print(f\"\\nSample merged data:\")\n",
    "print(df[['ppstaten', 'state_name', 'pce_pct_change', 'real_income_2023']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "features_header",
   "metadata": {},
   "source": [
    "## Select Features\n",
    "\n",
    "Based on SHED 2024 codebook, selecting key demographic and economic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "select_features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available features: 11 out of 11\n",
      "Missing features: set()\n",
      "\n",
      "Feature matrix shape: (12295, 11)\n",
      "Target shape: (12295,)\n"
     ]
    }
   ],
   "source": [
    "feature_columns = [\n",
    "    'ppage',           # Age\n",
    "    'pphhsize',        # Household size\n",
    "    'ppkid017',        # Number of children under 18\n",
    "    'ppeducat',        # Education level (4 categories)\n",
    "    'ppethm',          # Race/ethnicity\n",
    "    'ppgender',        # Gender\n",
    "    'ppinc7',          # Household income (7 categories)\n",
    "    \n",
    "    'EF1',             # 3-month emergency fund (Yes/No)\n",
    "    \n",
    "    'pce_pct_change',\n",
    "    'income_pct_change',\n",
    "    'real_income_2023',\n",
    "]\n",
    "\n",
    "available_features = [col for col in feature_columns if col in df.columns]\n",
    "print(f\"Available features: {len(available_features)} out of {len(feature_columns)}\")\n",
    "print(f\"Missing features: {set(feature_columns) - set(available_features)}\")\n",
    "\n",
    "X = df[available_features].copy()\n",
    "y = df['can_cover_400'].copy()\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a432d816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting categorical columns in df...\n",
      "Education: 12295 values converted\n",
      "Race: 12295 values converted\n",
      "Gender: 12295 values converted\n",
      "Income: 12295 values converted\n",
      "EF1: 12295 values converted\n",
      "\n",
      "✓ All categorical variables converted to numeric!\n",
      "\n",
      "Feature matrix: (12295, 11)\n",
      "Missing values in X: 603\n",
      "\n",
      "Missing by column:\n",
      "pce_pct_change       201\n",
      "income_pct_change    201\n",
      "real_income_2023     201\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Converting categorical columns in df...\")\n",
    "\n",
    "education_mapping = {\n",
    "    'No high school diploma or GED': 1,\n",
    "    'High school graduate (high school diploma or the equivalent GED)': 2,\n",
    "    \"Some college or Associate's degree\": 3,\n",
    "    \"Bachelor's degree or higher\": 4\n",
    "}\n",
    "df['ppeducat'] = df['ppeducat'].map(education_mapping)\n",
    "print(f\"Education: {df['ppeducat'].notna().sum()} values converted\")\n",
    "\n",
    "race_mapping = {\n",
    "    'White, Non-Hispanic': 1,\n",
    "    'Black, Non-Hispanic': 2,\n",
    "    'Other, Non-Hispanic': 3,\n",
    "    'Hispanic': 4,\n",
    "    '2+ Races, Non-Hispanic': 5\n",
    "}\n",
    "df['ppethm'] = df['ppethm'].map(race_mapping)\n",
    "print(f\"Race: {df['ppethm'].notna().sum()} values converted\")\n",
    "\n",
    "gender_mapping = {'Male': 1, 'Female': 2}\n",
    "df['ppgender'] = df['ppgender'].map(gender_mapping)\n",
    "print(f\"Gender: {df['ppgender'].notna().sum()} values converted\")\n",
    "\n",
    "income_mapping = {\n",
    "    'Less than $10,000': 1,\n",
    "    '$10,000 to $24,999': 2,\n",
    "    '$25,000 to $49,999': 3,\n",
    "    '$50,000 to $74,999': 4,\n",
    "    '$75,000 to $99,999': 5,\n",
    "    '$100,000 to $149,999': 6,\n",
    "    '$150,000 or more': 7\n",
    "}\n",
    "df['ppinc7'] = df['ppinc7'].map(income_mapping)\n",
    "print(f\"Income: {df['ppinc7'].notna().sum()} values converted\")\n",
    "\n",
    "df['EF1'] = (df['EF1'] == 'Yes').astype(int)\n",
    "print(f\"EF1: {df['EF1'].notna().sum()} values converted\")\n",
    "\n",
    "print(\"\\n✓ All categorical variables converted to numeric!\")\n",
    "\n",
    "feature_columns = [\n",
    "    'ppage', 'pphhsize', 'ppkid017', 'ppeducat', 'ppethm', 'ppgender', 'ppinc7',\n",
    "    'EF1', 'pce_pct_change', 'income_pct_change', 'real_income_2023'\n",
    "]\n",
    "\n",
    "X = df[feature_columns].copy()\n",
    "y = df['can_cover_400'].copy()\n",
    "\n",
    "print(f\"\\nFeature matrix: {X.shape}\")\n",
    "print(f\"Missing values in X: {X.isnull().sum().sum()}\")\n",
    "print(\"\\nMissing by column:\")\n",
    "print(X.isnull().sum()[X.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing_header",
   "metadata": {},
   "source": [
    "## Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "handle_missing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before handling:\n",
      "pce_pct_change       201\n",
      "income_pct_change    201\n",
      "real_income_2023     201\n",
      "dtype: int64\n",
      "\n",
      "Records after removing missing target: 12295\n",
      "pce_pct_change: Filled 0 missing with median 2.60\n",
      "income_pct_change: Filled 0 missing with median 1.90\n",
      "real_income_2023: Filled 0 missing with median 581816.00\n",
      "\n",
      "Dataset shape after cleaning: (12295, 11)\n",
      "Missing values after handling: 0\n",
      "\n",
      "✓ All missing values handled successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing values before handling:\")\n",
    "print(X.isnull().sum()[X.isnull().sum() > 0])\n",
    "\n",
    "valid_indices = ~y.isnull()\n",
    "X = X[valid_indices]\n",
    "y = y[valid_indices]\n",
    "\n",
    "print(f\"\\nRecords after removing missing target: {len(X)}\")\n",
    "\n",
    "for col in X.columns:\n",
    "    if X[col].isnull().sum() > 0:\n",
    "        median_val = X[col].median()\n",
    "        X[col].fillna(median_val, inplace=True)\n",
    "        print(f\"{col}: Filled {X[col].isnull().sum()} missing with median {median_val:.2f}\")\n",
    "\n",
    "print(f\"\\nDataset shape after cleaning: {X.shape}\")\n",
    "print(f\"Missing values after handling: {X.isnull().sum().sum()}\")\n",
    "\n",
    "if X.isnull().sum().sum() == 0:\n",
    "    print(\"\\n✓ All missing values handled successfully!\")\n",
    "else:\n",
    "    print(\"\\n✗ Still have missing values!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split_header",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "train_test_split",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 9,836 samples\n",
      "Test set: 2,459 samples\n",
      "\n",
      "Class distribution in train:\n",
      "can_cover_400\n",
      "0    0.56344\n",
      "1    0.43656\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class distribution in test:\n",
      "can_cover_400\n",
      "0    0.563237\n",
      "1    0.436763\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
    "print(f\"\\nClass distribution in train:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(f\"\\nClass distribution in test:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scale_header",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "feature_scaling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features scaled using StandardScaler\n",
      "Training set shape: (9836, 11)\n",
      "Test set shape: (2459, 11)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Features scaled using StandardScaler\")\n",
    "print(f\"Training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Test set shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_header",
   "metadata": {},
   "source": [
    "## Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "save_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to ../data/processed/\n",
      "\n",
      "Preprocessing Summary:\n",
      "- Features: 11\n",
      "- Training samples: 9,836\n",
      "- Test samples: 2,459\n",
      "- Target: can_cover_400 (binary)\n",
      "- Can cover $400: 43.7%\n",
      "- Cannot cover $400: 56.3%\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "np.save('../data/processed/X_train_scaled.npy', X_train_scaled)\n",
    "np.save('../data/processed/X_test_scaled.npy', X_test_scaled)\n",
    "np.save('../data/processed/y_train.npy', y_train)\n",
    "np.save('../data/processed/y_test.npy', y_test)\n",
    "\n",
    "with open('../data/processed/feature_names.pkl', 'wb') as f:\n",
    "    pickle.dump(X.columns.tolist(), f)\n",
    "with open('../data/processed/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"Processed data saved to ../data/processed/\")\n",
    "print(\"\\nPreprocessing Summary:\")\n",
    "print(f\"- Features: {X_train_scaled.shape[1]}\")\n",
    "print(f\"- Training samples: {X_train_scaled.shape[0]:,}\")\n",
    "print(f\"- Test samples: {X_test_scaled.shape[0]:,}\")\n",
    "print(f\"- Target: can_cover_400 (binary)\")\n",
    "print(f\"- Can cover $400: {y.mean()*100:.1f}%\")\n",
    "print(f\"- Cannot cover $400: {(1-y.mean())*100:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
